{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5rfZ_hYaQ1Z"
   },
   "source": [
    "<center>\n",
    "    <img src=\"http://sct.inf.utfsm.cl/wp-content/uploads/2020/04/logo_di.png\" style=\"width:60%\">\n",
    "    <h1> INF-395/477 Redes Neuronales Artificiales I-2020 </h1>\n",
    "    <h3>  Tarea 3 - Redes Neuronales Convolucionales y Recurrentes  </h3>\n",
    "</center>\n",
    "\n",
    "Nombres: Leonardo Astudillo Villalon  & Camilo Nunez Fernandez\n",
    "\n",
    "Roles: 201573598-0 &201573573-5\n",
    "\n",
    "Correos:\n",
    "leonardo.astudillov@sansano.usm.cl & camilo.nunezf@sansano.usm.cl\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "\n",
    "**Formalidades**  \n",
    "* Equipos de trabajo de: 3 personas (*cada uno debe estar en condiciones de realizar una presentación y discutir sobre cada punto del trabajo realizado*)\n",
    "* Formato de entrega: envı́o de link Github y link de video Youtube o plataforma a convenir, todo esto vía Aula. \n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "### **Propuesta**\n",
    "* Se debe preparar una presentación de **15 a 20 minutos** donde se explique el cómo se va a realizar/resolver el taller, la metodología o propuesta de las componentes a experimentar y explorar. Más detalles en el Syllabus.\n",
    "* Fecha de encuentro Zoom: 24 de Julio en horario de clases.\n",
    "* Fecha de entrega de vídeo: Opcional para quienes presentaron y obligatorio para quienes no, a lo más 2 días después del encuentro.\n",
    "* Modalidad de Presentación (Zoom): En el primer bloque, se formarán 3 grupos para que alcancen a recibir feedback todos los equipos. En el segundo bloque, algunos equipos seleccionados presentarán a todo el curso. \n",
    "\n",
    "**Aún si la idea es aprender colaborativamente, valoraremos mucho la diversidad de ideas, por lo que las propuesta debiesen conservar su orientación inicial, excepto por el feedback que les entreguemos**\n",
    "\n",
    "### **Defensa**\n",
    "* Se debe preparar una presentación de **15 a 20 minutos** con los resultados obtenidos y conclusiones de la experiencia. \n",
    "* Se debe entregar el código, de preferencia en un (breve) Jupyter/IPython notebook, de modo que **permita reproducir los resultados** presentados. Si se entrega el código fuente se deben proveer instrucciones para su uso.\n",
    "* Fecha de encuentro Zoom: 7 de Agosto, horario de clases.\n",
    "* Fecha de entrega de vídeo y Jypter (notebook): 7 de Agosto.\n",
    "* Modalidad de Presentación (Zoom): En ambos bloques algunos equipos seleccionados presentarán ante todo el curso, discusión y debate se generará en base a los resultados.\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Desafío por Método  \n",
    "[2.](#segundo) Desafío por Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuudGKMiAj3Z"
   },
   "source": [
    "#### <a id=\"primero\"></a>\n",
    "## 1. Desafío por Método: Neural Information Retrieval\n",
    "\n",
    "<img src=\"https://sites.google.com/site/xinchaoliweb/_/rsrc/1453028120793/research/ObjectRetrieval.jpg\" title=\"Title text\" width=\"80%\" />\n",
    "\n",
    "La tarea de un *sistema de recuperación de información (IR)* consiste en entregar los elementos de una cierta base de datos que resultan más relevantes para una determinada \"consulta\" (query) al sistema. Esta \"consulta\" se entrega al sistema en un formato pre-acordado con el usuario: por ejemplo podría tratarse de la conjunción de ciertas palabras claves.\n",
    "\n",
    "**Recuperación por Contenido:** En muchas aplicaciones, la base de datos está formada por objetos poco estructurados y ricos en contenido como una imagen, un video o una canción, e interesa poder recuperar los objetos más \"similares\" a una determinada query en términos semánticos. En este caso, más que términos claves, es práctico que la query pueda ser del mismo tipo de los **objetos** indexados: una imagen para recuperar imágenes o una canción para recuperar canciones. En este caso, el IR debe aprender qué objetos son los relevantes para una query porque la búsqueda se hace ya no guiada por términos claves o por instrucciones muy estructuradas, sino por contenido. Esto es aplicado a diferentes dominios: texto, imágenes, audio, vídeos. \n",
    "\n",
    "**Recuperación Inter-dominio:** En el problema de recuperación de información inter-dominio (**cross-modal** ó **cross-domain**), el objetivo es recuperar los objetos similares a un objeto de otro dominio. Por ejemplo, desde un texto que describe una situación, recuperar las imágenes similares (*Text2Image*), o desde una imagen recuperar los textos más similares a la situación en la imagen (*Image2Text*). Este último problema (Image2Text) puede ser similar a lo que se hace en Image Captioning pero el objetivo no es generar un texto nuevo, sino que se deben encontrar en un conjunto bajo una cierta similaridad.\n",
    "\n",
    "<img src=\"http://www.svcl.ucsd.edu/projects/crossmodal/images/problem.png\" title=\"Title text\" width=\"50%\" />\n",
    "\n",
    "Su tarea será utilizar redes neuronales para construir un IR basado en contenido del tipo\n",
    "1. mono-modal *image-2-image*  \n",
    "2. bi-modal *text-2-image*  \n",
    "3. bi-modal *image-2-text*  \n",
    "> **Importante**: Deberá escoger y resolver 2 de los 3 problemas \n",
    "\n",
    "Para testear y validar su solución tendrá a disposición diferentes datasets.\n",
    "Para el problema 1 puede utilizar cualquier dataset de imágenes RGB etiquetados que conozca (por ejemplo CIFAR) y construir los pares de imágenes en base a las categorías del problema: si comparten la misma clase se puede crear el par (imagen1-clase-K,imagen2-clase-K). Para el problema 2 y 3 utilice el dataset **Flickr8k**, una colección medianamente grande de imágenes que contiene 3 breves descripciones por cada una. Para entrenar puede utilizar sólo los datos de **Flickr8k** o algún otro dataset, con pares del tipo (imagen,texto), para complementar su propuesta.\n",
    "\n",
    "> Flickr8k: https://github.com/jbrownlee/Datasets/releases/tag/Flickr8k  \n",
    "> También podría utilizar algún otro como MSCOCO: https://cocodataset.org/#captions-2015  \n",
    "\n",
    "Recuerde que en fase de test, el sistema debe recibir una consulta y devolver una lista de resultados, que podemos asumir serán una serie de identificadores sobre la base de datos, por ejemplo: 2030, 3, 10, 120, 617.\n",
    "\n",
    "Su presentación de la solución debe incluir una clara descripción de (1) cómo se representan las consultas/objetos para ingresar al sistema, (2) cómo \n",
    "se entrena el modelo, (3) cómo evalúa el resultado final.\n",
    "\n",
    "* **Bonus** Se asigná un bonus de hasta 20 puntos sobre toda la tarea, a quienes exploren eventuales **sesgos de género** en el modelo, es decir, si la recuperación de los objetos evidencia diferencias significativas entre imágenes masculinas o femeninas (esas están anotadas). Por ejemplo, cuando ante la palabra \"conducir\" aperecen muchas más imágenes con personas clasificadas como masculino ó bien cuando la palabra \"woman\" o \"man\" se encuentra en los textos, se recuperan imágenes con un cierto *sesgo* (*bias*) visual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nDWquLhAj7q"
   },
   "source": [
    "#### <a id=\"segundo\"></a>\n",
    "## 2. Desafío por Score: Modelo Multi-tareas en Audio\n",
    "\n",
    "En esta sección se trabajará en el dominio de audio, como solicitaron. En particular se contará con audios de llantos de bebés, los cuales habrá que clasificar en base a diferentes categorías. Para hacer el problema mucho más desafiante el objetivo será **multi-task**, es decir, predecir múltiples tareas y objetivos simultáneamente (nos interesa que aprendan a construir una arquitectura para este tipo de problemas).\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/9l3tjzgyn9gr/photo-61067/c52112b6d064a0a5479bc520c89d5865/61067-babycrt.jpg?fm=jpg&fl=progressive&q=50&w=1200\" title=\"Title text\" width=\"30%\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eNo1Qc4X61FY"
   },
   "source": [
    "* Link Kaggle: Pendiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwPYaHsK5otT"
   },
   "source": [
    "El dataset mencionado fue recolectado con la ayuda de padres que mandaron el audio de sus bebés etiquetando ellos mismo la causa. Se contará con (i) predicción de la \"causa'' del llanto, (ii) rango etario y (iii) sexo. La métrica de evaluación tendrá en cuenta las 3 tareas.\n",
    "\n",
    "* Causas del llanto: *hungry, needs burping, belly pain, discomfort, tired, lonely,cold/hot, scared,don't know*\n",
    "\n",
    "El dataset disponible es pequeño, desbalanceado y, como decíamos, corresponde a audio. Hay muchos modos de representar el audio. Por ejemplo, si toma la Transformada de Fourier Reducida (Short-time Fourier Transform, STFT) de los audios o extrae Mel Frequency Cepstral Coefficients (MFCC), obtendrá imágenes, pudiendo aplicar todo lo que ha aprendido de ese dominio. El desbalanceo y el pequeño tamaño del dataset se puede manejar haciendo **data augmentation** o de otro modo que usted proponga. Finalmente, para manejar la escacez de datos de entrenamiento podría utilizar alguna de estas ideas: (i) red pre-entrenada (ya sabemos que funciona), (ii) expansión del dataset con otro dataset que usted consiga de otra parte \n",
    "\n",
    "1.   Por ejemplo acá hay audios de bebés riendo y bebés llorando: https://research.google.com/audioset/dataset/baby_laughter.html\n",
    "2.   Acá hay audios de bebés y otros animales: https://github.com/karolpiczak/ESC-50\n",
    "3. Acá hay audios de llantos de bebés, vidrio quebrado y disparos: http://www.cs.tut.fi/sgn/arg/dcase2017/challenge/task-rare-sound-event-detection#audio-dataset\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WbaNu3DwK80"
   },
   "source": [
    "# Image 2 Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTGZKG1kAWNd"
   },
   "source": [
    "## Download Flickr8-30k, COCO, vse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "colab_type": "code",
    "id": "J1NZmHTgCFLI",
    "outputId": "3e5b11de-4869-42cb-f620-10112de5680e"
   },
   "outputs": [],
   "source": [
    "!wget http://www.cs.toronto.edu/~rkiros/datasets/f8k.zip\n",
    "!wget http://www.cs.toronto.edu/~rkiros/datasets/f30k.zip\n",
    "!wget http://www.cs.toronto.edu/~rkiros/datasets/coco.zip\n",
    "!wget http://www.cs.toronto.edu/~rkiros/models/vse.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "colab_type": "code",
    "id": "ak8yiurFCIm5",
    "outputId": "da35100e-650b-4b55-997a-6eafc944d0ad"
   },
   "outputs": [],
   "source": [
    "!unzip f8k.zip -d f8k\n",
    "!unzip f30k.zip -d f30k\n",
    "!unzip coco.zip -d coco\n",
    "!unzip vse.zip -d vse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "te0Y9P2fAVUb"
   },
   "outputs": [],
   "source": [
    "#!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
    "#!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
    "#!unzip Flickr8k_Dataset.zip -d all_images\n",
    "#!unzip Flickr8k_text.zip -d all_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SA5KsuSVxwCG"
   },
   "source": [
    "Cargar la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KkZ3YAsNrpgB"
   },
   "outputs": [],
   "source": [
    "!mkdir dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AQIhUlO7vIO"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "W_4dJphvHVqm",
    "outputId": "4f2ca170-55b5-4754-d62d-9ae13cc698ff"
   },
   "outputs": [],
   "source": [
    "!mv  -v /content/f8k /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PHQ-NxhXYjz"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftpOMA3vXa12"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "from random import shuffle\n",
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wnMOvnQt5H5J"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mf9TfHs_0iND"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "\t\"num_epochs\":7,\n",
    "\t\"batch_size\":128,\n",
    "\t\"word_dimension\":1000, # The dimensionality of word embeddings\n",
    "\t\"image_dimension\":4096, # input image dimension : 4096 for VGG, 512 resnet\n",
    "\t\"model_dimension\":1000, # The dimension of the embedding space,\n",
    "\t\"learning_rate\":0.01,\n",
    "\t\"display_freq\":50, # how often to display loss : 1 = every batch, 2 = every second batch etc...\n",
    "\t\"margin_pairwise_ranking_loss\":0.2, # Should be between zero and 1,\n",
    "\t\"dataset\":\"f8k\", # visual_fashion_dialog, deepfashion\n",
    "\t\"cuda\":True # enable cuda\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJXt4dDY5J-V"
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DKoUDu_A2w-e"
   },
   "outputs": [],
   "source": [
    "class PairwiseRankingLoss(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(PairwiseRankingLoss, self).__init__()\n",
    "\t\tself.margin = config[\"margin_pairwise_ranking_loss\"]\n",
    "\n",
    "\tdef forward(self, sentence, image):\t\t\t\t\n",
    "\t\tmargin = self.margin\t\t\n",
    "\t\tscores = torch.mm(image, sentence.transpose(1, 0))\n",
    "\t\tdiagonal = scores.diag()\n",
    "\n",
    "\t\tsentence_cost = None\t\t\n",
    "\t\timage_cost = None\n",
    "\n",
    "\t\tif torch.cuda.is_available() and config[\"cuda\"] == True:\n",
    "\t\t\tsentence_cost = torch.max(Variable(torch.zeros(scores.size()[0], scores.size()[1])).cuda(), (self.margin-diagonal).expand_as(scores)+scores)\t\t\n",
    "\t\t\timage_cost = torch.max(Variable(torch.zeros(scores.size()[0], scores.size()[1])).cuda(), (self.margin-diagonal).expand_as(scores).transpose(1, 0)+scores)\n",
    "\t\telse:\n",
    "\t\t\tsentence_cost = torch.max(Variable(torch.zeros(scores.size()[0], scores.size()[1])), (margin-diagonal).expand_as(scores)+scores)\n",
    "\t\t\timage_cost = torch.max(Variable(torch.zeros(scores.size()[0], scores.size()[1])), (margin-diagonal).expand_as(scores).transpose(1, 0)+scores)\n",
    "\n",
    "\t\tfor i in range(scores.size()[0]):\n",
    "\t\t\tsentence_cost[i, i] = 0\n",
    "\t\t\timage_cost[i, i] = 0\n",
    "\n",
    "\t\treturn sentence_cost.sum() + image_cost.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9RvoyRnK5LYl"
   },
   "source": [
    "##Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tStEO_WI3QEl"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer:\n",
    "\tdef __init__(self, model, learning_rate=0.0002):\n",
    "\t\tself.learning_rate = config[\"learning_rate\"]\n",
    "\t\tself.params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\t\tself.optimizer = torch.optim.Adam(self.params, learning_rate)\n",
    "\t\tself.display_freq = config[\"display_freq\"]\n",
    "\t\tself.display_count = 0\n",
    "\t\t\n",
    "\tdef backprop(self, cost, grad_clip=2.0):\t\t\n",
    "\t\tif self.display_count % self.display_freq == 0 or self.display_count == 0:\n",
    "\t\t\tprint(\"\t* Cost:\", cost.data.cpu().numpy())\t\t\n",
    "\t\tself.display_count += 1\n",
    "\t\tself.optimizer.zero_grad() # Reset gradient\n",
    "\t\tcost.backward() # Back propagate\t\t\n",
    "\t\ttorch.nn.utils.clip_grad_norm_(self.params, grad_clip)\n",
    "\t\tself.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ep9sD_Xj6IsG"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KOLQijy6LD3"
   },
   "outputs": [],
   "source": [
    "def image_to_text(captions, images, npts=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Images->Text (Image Annotation)\n",
    "    Images: (5N, K) matrix of images\n",
    "    Captions: (5N, K) matrix of captions\n",
    "    \"\"\"\n",
    "    if npts == None:\n",
    "        npts = images.size()[0] / 5\n",
    "        npts = int(npts)\n",
    "\n",
    "    ranks = numpy.zeros(npts)\n",
    "    for index in range(npts):\n",
    "\n",
    "        # Get query image\n",
    "        im = images[5 * index].unsqueeze(0)\n",
    "\n",
    "        # Compute scores\n",
    "        d = torch.mm(im, captions.t())\n",
    "        d_sorted, inds = torch.sort(d, descending=True)\n",
    "        inds = inds.data.squeeze(0).cpu().numpy()\n",
    "\n",
    "        # Score\n",
    "        rank = 1e20\n",
    "        # find the highest ranking\n",
    "        for i in range(5*index, 5*index + 5, 1):\n",
    "            tmp = numpy.where(inds == i)[0][0]\n",
    "            if tmp < rank:\n",
    "                rank = tmp\n",
    "        ranks[index] = rank\n",
    "\n",
    "    # Compute metrics\n",
    "    print(numpy.where(ranks < 1)[0])\n",
    "    print(numpy.where(ranks < 5)[0])\n",
    "    print(numpy.where(ranks < 10)[0])\n",
    "    r1 = 100.0 * len(numpy.where(ranks < 1)[0]) / len(ranks)\n",
    "    r5 = 100.0 * len(numpy.where(ranks < 5)[0]) / len(ranks)\n",
    "    r10 = 100.0 * len(numpy.where(ranks < 10)[0]) / len(ranks)\n",
    "    medr = numpy.floor(numpy.median(ranks)) + 1\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\t\t* Image to text scores: R@1: %.1f, R@5: %.1f, R@10: %.1f, Medr: %.1f\" % (r1, r5, r10, medr))\n",
    "    return r1+r5+r10, (r1, r5, r10, medr)\n",
    "\n",
    "def text_to_image(captions, images, npts=None, verbose=False):\n",
    "    if npts == None:\n",
    "    \tnpts = images.size()[0] / 5\n",
    "    \tnpts = int(npts)\n",
    "\n",
    "    ims = torch.cat([images[i].unsqueeze(0) for i in range(0, len(images), 5)])\n",
    "\n",
    "    ranks = numpy.zeros(5 * npts)\n",
    "    for index in range(npts):\n",
    "\n",
    "        # Get query captions\n",
    "        queries = captions[5*index : 5*index + 5]\n",
    "\n",
    "        # Compute scores\n",
    "        d = torch.mm(queries, ims.t())\n",
    "        for i in range(d.size()[0]):\n",
    "            d_sorted, inds = torch.sort(d[i], descending=True)\n",
    "            inds = inds.data.squeeze(0).cpu().numpy()\n",
    "            ranks[5 * index + i] = numpy.where(inds == index)[0][0]\n",
    "\n",
    "    # Compute metrics\n",
    "    r1 = 100.0 * len(numpy.where(ranks < 1)[0]) / len(ranks)\n",
    "    r5 = 100.0 * len(numpy.where(ranks < 5)[0]) / len(ranks)\n",
    "    r10 = 100.0 * len(numpy.where(ranks < 10)[0]) / len(ranks)\n",
    "    medr = numpy.floor(numpy.median(ranks)) + 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\t\t* Text to image scores: R@1: %.1f, R@5: %.1f, R@10: %.1f, Medr: %.1f\" % (r1, r5, r10, medr))\n",
    "    return r1+r5+r10, (r1, r5, r10, medr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvOc0uA05PTN"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2N9bS-IN2NBU"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\tdef __init__(self, data):\n",
    "\t\tsuper(Model, self).__init__()\n",
    "\n",
    "\t\t# Performance score\n",
    "\t\tself.score = 0\n",
    "\t\tself.best_score = 0\n",
    "\n",
    "\t\t# Filename\n",
    "\t\tself.input_name = \"best\"\n",
    "\t\tself.output_name = \"best\"\n",
    "\n",
    "\t\t# number of words in dictionary\n",
    "\t\tnum_words = len(data.word_to_index)\n",
    "\n",
    "\t\t# Sentence\n",
    "\t\tself.embedding = torch.nn.Embedding(num_words, config['word_dimension'])\n",
    "\t\tself.lstm = torch.nn.LSTM(config['word_dimension'], config['model_dimension'], 1)\n",
    "\n",
    "\t\t# Image - Assume image feature is already extracted from pre-trained CNN\n",
    "\t\tself.linear = torch.nn.Linear(config['image_dimension'], config['model_dimension'])\n",
    "\n",
    "\t\t# Initialize weights for linear layer\n",
    "\t\ttorch.nn.init.xavier_uniform_(self.linear.weight)\t\t\n",
    "\t\tself.linear.bias.data.fill_(0)\t\t\n",
    "\n",
    "\t\tif torch.cuda.is_available() and config[\"cuda\"] == True:\n",
    "\t\t\tself.embedding.cuda()\n",
    "\t\t\tself.lstm.cuda()\n",
    "\t\t\tself.linear.cuda()\t\t\n",
    "\n",
    "\tdef forward(self, sentence, image):\t\t\n",
    "\t\treturn self.forward_caption(sentence), self.forward_image(image)\n",
    "\n",
    "\tdef forward_image(self, image):\n",
    "\t\t# Pass image through model\n",
    "\t\timage_embedding = self.linear(image)\n",
    "\n",
    "\t\t# Normalize\n",
    "\t\tnorm_image_embedding = F.normalize(image_embedding, p=2, dim=1)\n",
    "\n",
    "\t\treturn norm_image_embedding\n",
    "\n",
    "\tdef forward_caption(self, sentence):\n",
    "\n",
    "\t\t# Pass caption through model\n",
    "\t\tsentence_embedding = self.embedding(sentence)\n",
    "\n",
    "\t\t_, (sentence_embedding, _) = self.lstm(sentence_embedding)\n",
    "\n",
    "\t\tx_sentence_embedding = sentence_embedding.squeeze(0)\n",
    "\n",
    "\t\t# Normalize vectors\n",
    "\t\tnorm_sentence_embedding = F.normalize(x_sentence_embedding, p=2, dim=1)\t\t\n",
    "\n",
    "\t\treturn norm_sentence_embedding\n",
    "\n",
    "\tdef average_i2t_and_t2i(self, i2t, t2i):\n",
    "\t\ti_r1, i_r5, i_r10, i_medr, t_r1, t_r5, t_r10, t_medr = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "\t\tfor x in i2t:\n",
    "\t\t\ti_r1 += x[0]\n",
    "\t\t\ti_r5 += x[1]\n",
    "\t\t\ti_r10 += x[2]\n",
    "\t\t\ti_medr += x[3]\n",
    "\n",
    "\t\tfor x in t2i:\n",
    "\t\t\tt_r1 += x[0]\n",
    "\t\t\tt_r5 += x[1]\n",
    "\t\t\tt_r10 += x[2]\n",
    "\t\t\tt_medr += x[3]\n",
    "\n",
    "\t\ti_r1 = i_r1/len(i2t)\n",
    "\t\ti_r5 = i_r5/len(i2t)\n",
    "\t\ti_r10 = i_r10/len(i2t)\n",
    "\t\ti_medr = i_medr/len(i2t)\n",
    "\n",
    "\t\tt_r1 = t_r1/len(i2t)\n",
    "\t\tt_r5 = t_r5/len(i2t)\n",
    "\t\tt_r10 = t_r10/len(i2t)\n",
    "\t\tt_medr = t_medr/len(i2t)\n",
    "\n",
    "\t\tprint(\"\t* Image to text scores: R@1: %.1f, R@5: %.1f, R@10: %.1f, Medr: %.1f\" % (i_r1, i_r5, i_r10, i_medr))\t\t\n",
    "\t\tprint(\"\t* Text to image scores: R@1: %.1f, R@5: %.1f, R@10: %.1f, Medr: %.1f\" % (t_r1, t_r5, t_r10, t_medr))\n",
    "\t\t\t\n",
    "\t\treturn\n",
    "\n",
    "\tdef evaluate(self, data, verbose=False, save_if_better=False):\n",
    "\t\t\"\"\"\n",
    "\t\tIf using k-fold cross validation in the data module,\n",
    "\t\tthe data class will handle updaing the self.train and self.test\n",
    "\t\tdatasets. Thus the data.test_set(True) becomes very important.\n",
    "\t\tHowever, a raw intialization of the dataclass with result in\n",
    "\t\tthe loaded data being assigned to both test and train, so we can\n",
    "\t\tevaluate the results. \n",
    "\t\t\"\"\"\n",
    "\t\tprint(\"\t* Validating\", end=\"\", flush=True)\t\t\t\t\n",
    "\t\tdata.test_set(True) # very important | swaps to iterating over the test set for validation\n",
    "\t\tscore = 0\n",
    "\t\ti2t, t2i = [], []\n",
    "\t\tfor caption, image_feature in data:\t\t\t\t\n",
    "\t\t\tx, y = self.forward(caption, image_feature)\n",
    "\t\t\tscore_1, i2t_result = image_to_text(x, y, verbose=verbose)\n",
    "\t\t\tscore_2, t2i_result = text_to_image(x, y, verbose=verbose)\t\n",
    "\t\t\tscore += (score_1 + score_2)\t\t\n",
    "\t\t\ti2t.append(i2t_result)\t\n",
    "\t\t\tt2i.append(t2i_result)\n",
    "\t\t\tprint(\".\", end=\"\", flush=True)\t\t\n",
    "\t\t\n",
    "\t\tprint(\"[DONE]\", end=\"\", flush=True)\n",
    "\t\tprint(\"\")\n",
    "\t\tdata.test_set(False) # also very important | swaps BACK to using the TRAIN set\n",
    "\t\tself.average_i2t_and_t2i(i2t, t2i)\n",
    "\n",
    "\t\tif save_if_better and score > self.best_score:\n",
    "\t\t\tself.save()\n",
    "\t\t\tdata.save_dictionaries()\n",
    "\t\t\tself.best_score = score\n",
    "\n",
    "\t\treturn score\t\n",
    "\n",
    "\tdef save(self):\n",
    "\t\tprint('\t* Saving model...')\t\t\t\n",
    "\t\ttorch.save(self.state_dict(), self.output_name+'.pkl')\n",
    "\t\tprint('\t* Done!')\n",
    "\t\treturn\n",
    "\n",
    "\tdef load(self):\t\t\n",
    "\t\tself.load_state_dict(torch.load(self.input_name+\".pkl\"))\n",
    "\t\tprint(\"[LOADED]\", self.input_name+\".pkl\", \"\\n\")\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcbGYG0f5Z85"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uba2jooXy7Kx"
   },
   "outputs": [],
   "source": [
    "class KFoldCrossValidation:\n",
    "    \"\"\"\n",
    "    Handles splitting data into folds and serving each train/test through a generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def split(self, a, n):\n",
    "        k, m = divmod(len(a), n)\n",
    "        return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "    def __init__(self, data, k):    \n",
    "\n",
    "    \t# set up for iteration    \n",
    "        self.k = k\n",
    "        self.idx = 0      \n",
    "        \n",
    "        # zip list and shuffle it\n",
    "        d = list(zip(data[0], data[1]))\n",
    "        shuffle(d)\n",
    "\n",
    "        # create split array\n",
    "        gen = self.split(d, k)\n",
    "        self.splits = []\n",
    "        for i in gen:\n",
    "        \tself.splits.append(i)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):    \t\n",
    "        if self.idx == self.k:\n",
    "            raise StopIteration\n",
    "\n",
    "        test = self.splits[self.idx]\n",
    "        shuffle(test)\n",
    "\n",
    "        train = []\n",
    "        for i, split in enumerate(self.splits):\n",
    "        \tif i != self.idx:\n",
    "        \t\tshuffle(split)\n",
    "        \t\ttrain += split\n",
    "\n",
    "        shuffle(train)\n",
    "\n",
    "        self.idx += 1\n",
    "\n",
    "        return train, test, self.idx\n",
    "\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    Will default to using k_fold cross validation.\n",
    "    Set the \"self.use_test_set\" to process the test set.\n",
    "    \"\"\"\n",
    "    def __init__(self, create_dict=False):\n",
    "\n",
    "        # Load captions as array of strings corresponding to an array of image feature vectors\n",
    "        self.load_dataset(name=config[\"dataset\"])\n",
    "        self.reset()\n",
    "\n",
    "        if create_dict:\n",
    "        \tself.create_dictionaries()\n",
    "\n",
    "    def k_folds(self, k=5):        \n",
    "        return KFoldCrossValidation(self.data, k)\n",
    "\n",
    "    def process(self, train, test, fold, create_dictionaries=True):\n",
    "        print(\"\\n[PROCESSING] fold\", fold) \n",
    "        self.train = list(zip(*train))\n",
    "        self.test = list(zip(*test))\n",
    "        self.reset()\n",
    "        if create_dictionaries:\n",
    "            self.create_dictionaries()\n",
    "\n",
    "    def reset(self):\n",
    "    \t# Reset counter & batch size\n",
    "        self.batch_size = config[\"batch_size\"]        \n",
    "        self.batch_number = 0\n",
    "        self.previous_batch_number = 0   \n",
    "        self.use_test_set = False     \n",
    "        return\n",
    "\n",
    "    def test_set(self, mode):\n",
    "        \"\"\"\n",
    "        Will switch to iterating over the test set or train set.\n",
    "        \"\"\"\n",
    "        if mode:\n",
    "            self.use_test_set = True\n",
    "            self.previous_batch_number = self.batch_number\n",
    "            self.batch_number = 0\n",
    "        else:\n",
    "            self.use_test_set = False\n",
    "            self.batch_number = self.previous_batch_number        \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):        \n",
    "        \"\"\"\n",
    "        Return a batch of data ready to go into the model\n",
    "        \"\"\"\n",
    "\n",
    "        data_set = self.train\n",
    "        if self.use_test_set:\n",
    "        \tdata_set = self.test      \t\n",
    "\n",
    "        # Upper and lower indexes for the batches\n",
    "        upper_idx = (self.batch_number+1)*self.batch_size\n",
    "        lower_idx = self.batch_number*self.batch_size\n",
    "\n",
    "        # If our lower index is in the bounds of our data, we can return a new batch\n",
    "        if lower_idx < len(data_set[0]):            \n",
    "            self.batch_number += 1 # Increment the batch number so next-time we will return the next batch\n",
    "            captions = data_set[0][lower_idx:upper_idx] # Extract caption batch\n",
    "            image_features = data_set[1][lower_idx:upper_idx] # Extract image feature batch\n",
    "            captions, image_features = self.preprocess_data(captions, image_features) # Preprocess our data, converting raw text to embedded numbers etc                        \n",
    "            return captions, image_features\n",
    "            \n",
    "        self.batch_number = 0\n",
    "        raise StopIteration\n",
    "    \n",
    "    def load_dataset(self, name='f8k', path_to_data = 'data/', test=False):\n",
    "        \"\"\"\n",
    "        Load captions and image features\n",
    "        \"\"\"            \n",
    "        # Original\n",
    "        #loc = path_to_data + name + '/'\n",
    "        # Custom\n",
    "        loc = path_to_data + name +  '/'\n",
    "        # Captions\n",
    "        captions = []\n",
    "        with open(loc+name+'_train_caps.txt','rb') as f:\n",
    "            for line in f:                                \n",
    "                captions.append(line.strip())\n",
    "\n",
    "        # Image features\n",
    "        ims = numpy.load(loc+name+'_train_ims.npy')\n",
    "                \n",
    "        self.data = (captions, ims)\n",
    "        self.train = self.data\n",
    "        self.test = self.data\n",
    "\n",
    "        print(\"[LOADED]\", name, \"dataset\") \n",
    "\n",
    "        if len(self.data[0]) != len(self.data[1]):\n",
    "            print(\"Captions do not match image features one to one for dataset!\")           \n",
    "\n",
    "        return\n",
    "    \n",
    "    def load_dictionaries(self):    \t\n",
    "        self.word_to_index = pickle.load(open('dict/word_to_index.pkl', 'rb'))\n",
    "        self.index_to_word = pickle.load(open('dict/index_to_word.pkl', 'rb'))\n",
    "        print(\"[LOADED] dictionaries from dict/\") \n",
    "        return\n",
    "\n",
    "    def save_dictionaries(self):\n",
    "        # Save dictionaries        \n",
    "        with open('dict/word_to_index.pkl', 'wb') as file:\n",
    "            pickle.dump(self.word_to_index, file)\n",
    "        with open('dict/index_to_word.pkl', 'wb') as file:\n",
    "            pickle.dump(self.index_to_word, file)\n",
    "        return    \t\n",
    "\n",
    "    def create_dictionaries(self):\n",
    "        \"\"\"\n",
    "        Create the dictionaries to go from a word to an index and an index to a words.\n",
    "        \"\"\"        \n",
    "        captions = self.train[0]\n",
    "\n",
    "        # TODO: Add beginning and end of setence tokens thats not zero\n",
    "        self.word_to_index = {'<blank>':0, '<sos>':1, '<eos>':2, '<unk>':3}\n",
    "        self.index_to_word = {0:'<blank>', 1:'<sos>', 2:'<eos>', 3:'<unk>'}\n",
    "        pad = len(self.word_to_index)\n",
    "\n",
    "        words = set()                \n",
    "        for idx, caption in enumerate(captions):  \n",
    "            for word in caption.split():  \n",
    "                words.add(word)                \n",
    "\n",
    "        for idx, word in enumerate(words):\n",
    "            self.word_to_index[word] = idx + pad\n",
    "            self.index_to_word[idx+pad] = word\n",
    "\n",
    "        print(\"[INIT] dictionaries\")\n",
    "        print(\"[CONTAINS]\", len(self.word_to_index)-2, \"words\")        \n",
    "        return\n",
    "\n",
    "    def preprocess_data(self, captions, image_features, n_words=10000):\n",
    "        \"\"\"\n",
    "        Convert raw data to go into the model.\n",
    "        \"\"\"               \n",
    "        # Convert a caption to an array of indexes of words from the dictionary, self.word_to_index  \n",
    "        sequences = []        \n",
    "        for idx, caption in enumerate(captions):\n",
    "            seq = [self.word_to_index[word] if word in self.word_to_index.keys() else 1 for word in caption.split()]\n",
    "            seq.insert(0, self.word_to_index[\"<sos>\"])\n",
    "            seq.append(self.word_to_index[\"<eos>\"])\n",
    "            sequences.append(seq)                    \n",
    "\n",
    "        sequence_lengths = [len(seq) for seq in sequences] # the lengths of all sequences in an array\n",
    "        processed_captions = numpy.zeros((max(sequence_lengths)+1, len(sequences))).astype('int64') # create matrix w/ biggest length of sequence by length of all sequences\n",
    "        for idx, seq in enumerate(sequences):\n",
    "            processed_captions[:sequence_lengths[idx], idx] = seq # populate matrix with sequences\n",
    "\n",
    "        # Just convert image features to numpy array\n",
    "        processed_image_features = numpy.asarray(image_features, dtype=numpy.float32)\n",
    "\n",
    "        if torch.cuda.is_available() and config[\"cuda\"] == True:\n",
    "            return Variable(torch.from_numpy(processed_captions)).cuda(), Variable(torch.from_numpy(processed_image_features)).cuda()\n",
    "        \n",
    "        return Variable(torch.from_numpy(processed_captions)), Variable(torch.from_numpy(processed_image_features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMalFGtA0Z9m"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yQW_2YhW0abh",
    "outputId": "fdbfd5cc-7164-4cd8-c5f6-a5db20283cbb"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = Data(create_dict=True)\n",
    "\n",
    "# Load model\n",
    "model = Model(data)\n",
    "\n",
    "# Model loss function\n",
    "loss = PairwiseRankingLoss()\n",
    "\n",
    "# Optimizer \n",
    "optimizer = Optimizer(model)\n",
    "\n",
    "# Begin epochs\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "  print(\"[EPOCH]\", epoch+1)\n",
    "\n",
    "  # Process batches\n",
    "  for caption, image_feature in data:\n",
    "    caption, image_feature = model(caption, image_feature)\n",
    "\n",
    "    # Compute loss\n",
    "    cost = loss(caption, image_feature)\t\t\t\n",
    "\n",
    "    # Zero gradient, Optimize loss, and perform back-propagation\n",
    "    optimizer.backprop(cost)\n",
    "\n",
    "  # Evaluate final model results | save model if better\t\t\t\t\n",
    "  model.evaluate(data, save_if_better=True)\n",
    "\n",
    "# Final evaluation - save if results are better\t\t\n",
    "print(\"\\nFinal evaluation:\")\n",
    "model.evaluate(data, save_if_better=True)\n",
    "\n",
    "print(\"\\n[SCRIPT] complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ot9amSlQ5cyV"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mJDS0amJ3cHV",
    "outputId": "02808fb2-a098-44d8-b2c4-2f09e51c2e3c"
   },
   "outputs": [],
   "source": [
    "data = Data()\t\n",
    "data.load_dictionaries() # very important - load dictionaries \n",
    "\n",
    "model = Model(data)\n",
    "model.input_name = \"best\" # specify save model name (best)\n",
    "model.load() # load the saved model weights\n",
    "\n",
    "model.evaluate(data) # evaluate the data\n",
    "\n",
    "#\treturn\n",
    "print(\"\\nScript done :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "38YkSi_KfmWz",
    "outputId": "9f2c41c8-6fe6-4d6d-edad-adb241851050"
   },
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-b8GDWZPAFnj"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "import pickle as pkl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3FiHqRUBAdzo"
   },
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = 'static/upload/'\n",
    "dump_path = 'vse/arch_server/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sTYfLkVmCFLw"
   },
   "outputs": [],
   "source": [
    "curr_model = {}\n",
    "\n",
    "curr_model['img_sen_model'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCnF3HuP_sbL"
   },
   "outputs": [],
   "source": [
    "def encode_images(model, IM):\n",
    "    \"\"\"\n",
    "    Encode images into the joint embedding space\n",
    "    \"\"\"\n",
    "    IM = Variable(torch.from_numpy(IM).cuda())\n",
    "    images = model['img_sen_model'].forward_image(IM)\n",
    "    # images = images.data.cpu().numpy()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "Pp0QWkBFBSZE",
    "outputId": "e1a09a6d-1b85-40e2-dac6-f75cd63af14c"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Scale([224, 224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "                         std = [ 0.229, 0.224, 0.225 ]),\n",
    "])\n",
    "\n",
    "resnet = models.resnet152(pretrained=True)\n",
    "resnet.fc = torch.nn.Dropout(p=0)\n",
    "resnet = resnet.eval()\n",
    "resnet = resnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1lcCm8xvY6L"
   },
   "outputs": [],
   "source": [
    "#query\n",
    "dump_path = 'vse/'\n",
    "\n",
    "k_input = 5\n",
    "\n",
    "\n",
    "img_name = \"salad\"\n",
    "upload_img = \"/content/salad.jpg\"\n",
    "\n",
    "sim_texts, sim_texts_url = [], []\n",
    "if img_name:\n",
    "\n",
    "  img_vec = image_transform(Image.open(upload_img).convert('RGB')).unsqueeze(0)\n",
    "  image_emb = encode_images(curr_model, resnet(Variable(img_vec.cuda())).data.cpu().numpy())\n",
    "  d = torch.mm(image_emb, texts_dump.t())\n",
    "  d_sorted, inds = torch.sort(d, descending=True)\n",
    "  inds = inds.data.squeeze(0).cpu().numpy()\n",
    "  sim_texts = np.array(texts_orig)[inds[:k_input]]\n",
    "  sim_texts_url = np.array(texts_url)[inds[:k_input]]\n",
    "  sim_texts, sim_texts_url = sim_texts.tolist(), sim_texts_url.tolist()\n",
    "\n",
    "print(sim_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDZIRJ2e3chN"
   },
   "source": [
    "# Mono Modal Image Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(y_train)\n",
    "y_train = enc.transform(y_train).toarray()\n",
    "\n",
    "y_test = enc.transform(y_test).toarray()\n",
    "\n",
    "x_train = (x_train.astype('float32')) / 255.0\n",
    "x_test = (x_test.astype('float32')) / 255.0\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, Reshape, MaxPooling2D, UpSampling2D, Flatten, BatchNormalization, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "### FeatureExtractor Model\n",
    "\n",
    "input_layer = Input(shape=(32, 32, 3, ))\n",
    "        \n",
    "a_one = Conv2D(64, (3,3), activation='relu', padding='same') (input_layer)\n",
    "a_three = Conv2D(64, (3,3), activation='relu', padding='same') (a_one)\n",
    "a_five = MaxPooling2D() (a_three)\n",
    "block_one = Dropout(0.25) (a_five)\n",
    "\n",
    "b_one = Conv2D(128, (3,3), activation='relu', padding='same') (block_one)\n",
    "b_three = Conv2D(128, (3,3), activation='relu', padding='same') (b_one)\n",
    "b_five = MaxPooling2D() (b_three)\n",
    "block_two = Dropout(0.25) (b_five)\n",
    "\n",
    "c_one = Conv2D(256, (3,3), activation='relu', padding='same') (block_two)\n",
    "c_three = Conv2D(256, (3,3), activation='relu', padding='same') (c_one)\n",
    "c_five = MaxPooling2D() (c_three)\n",
    "block_three = Dropout(0.5) (c_five)\n",
    "\n",
    "d_one = Conv2D(512, (3,3), activation='relu', padding='same') (block_three)\n",
    "d_three = Conv2D(512, (1,1), activation='relu', padding='same') (d_one)\n",
    "d_five = MaxPooling2D() (d_three)\n",
    "block_four = Dropout(0.2) (d_five)\n",
    "\n",
    "flat = Flatten() (block_four)\n",
    "fc_one = Dense(4096, activation='relu') (flat)\n",
    "fc_two = Dense(4096, activation='relu') (fc_one)\n",
    "\n",
    "final = Dense(10, activation='softmax') (fc_two)\n",
    "\n",
    "FeatureExtractor = Model(input_layer, final)\n",
    "feature_extractor = Model(input_layer, flat)\n",
    "#FeatureExtractor.summary()\n",
    "FeatureExtractor.compile(optimizer=Adam(lr=0.0001, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "### Train FeatureExtractor Model\n",
    "FeatureExtractor.fit(x_train, y_train,epochs=100, batch_size=128, shuffle=True,  validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(FeatureExtractor, show_shapes=False, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feature_extractor.predict(x_train)\n",
    "features_test = feature_extractor.predict(x_test)\n",
    "\n",
    "print(features.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "input_dim = 2048\n",
    "#input_dim = 10\n",
    "encoding_dim = 128\n",
    "\n",
    "# DeepAutoencoder Models\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "hidden_one = Dense(encoding_dim*4, activation='relu') (input_layer)\n",
    "hidden_two = Dense(encoding_dim*2, activation='relu') (hidden_one)\n",
    "encoder_output = Dense(encoding_dim, activation='relu') (hidden_two)\n",
    "encoder_model = Model(input_layer, encoder_output)\n",
    "\n",
    "hidden_three = Dense(encoding_dim*2, activation='relu') (encoder_output)\n",
    "hidden_four = Dense(encoding_dim*4, activation='relu') (hidden_three)\n",
    "decoder_output = Dense(input_dim, activation='sigmoid') (hidden_four)\n",
    "autoencoder = Model(input_layer, decoder_output)\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy')\n",
    "\n",
    "### Train DeepAutoencoder Model\n",
    "autoencoder.fit(features, features,epochs=50, batch_size=256, shuffle=True, validation_data=(features_test, features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(autoencoder, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feature_extractor.predict(x_train)\n",
    "print(features.shape)\n",
    "vectors = encoder_model.predict(features)\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "index = AnnoyIndex(vectors.shape[1],metric='angular')\n",
    "for i in range(vectors.shape[0]):\n",
    "    index.add_item(i, vectors[i,:].tolist())\n",
    "    \n",
    "index.build(20)\n",
    "index.save(\"index.ann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "lookup = np.argmax(y_train[:, :], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "choice_idx = random.randrange(20000)\n",
    "results = index.get_nns_by_item(choice_idx, 10)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(1,4))\n",
    "plt.imshow(x_train[choice_idx])\n",
    "plt.axis('off')\n",
    "plt.title('query: ' + str(labels[lookup[choice_idx]]))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(1, 10, i+1)\n",
    "    ax.imshow(x_train[results[i]])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(str(labels[lookup[results[i]]]))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "L5rfZ_hYaQ1Z",
    "LuudGKMiAj3Z",
    "2nDWquLhAj7q",
    "iTGZKG1kAWNd",
    "4PHQ-NxhXYjz",
    "wnMOvnQt5H5J",
    "pJXt4dDY5J-V",
    "9RvoyRnK5LYl",
    "Ep9sD_Xj6IsG",
    "uvOc0uA05PTN",
    "mcbGYG0f5Z85",
    "sMalFGtA0Z9m",
    "ot9amSlQ5cyV"
   ],
   "name": "[ANN]Taller3-Image2Text.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
